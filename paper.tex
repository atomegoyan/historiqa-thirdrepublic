% LREC 2026 Example; 
% LREC Is now using templates similar to the ACL ones. 
\documentclass[10pt, a4paper]{article}

\usepackage[review]{lrec2026} % this is the new style
% the 'review' option anonymizes the paper following submission guideline
% the 'final' option produces the camera ready version (non anonymized)
% default version is 'final', so use review option for submission

%%% ADDED PACKAGES
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{caption}
\usepackage{rotating}
\usepackage{float}
\usepackage{comment}

\usepackage{adjustbox}
\usepackage{tabularx}

\usepackage{subcaption}


\definecolor{embeddingbg}{HTML}{F0F8FF} % light blue for embedding
\definecolor{rerankingbg}{HTML}{FFF5E5} % light orange for reranking

%\usepackage{booktabs}
%\usepackage{siunitx}
%\sisetup{
%    detect-weight=true,
%    detect-inline-weight=math,
%    table-number-alignment=center,
%    table-format=4.0,
%    table-space-text-post = \%,
%}

%%% VARIABLES
\newcommand{\tval}{0.7}
\newcommand{\nMHqueries}{885} % AUCUNE question removed
\newcommand{\nSHqueries}{897}
\newcommand{\nqueries}{1874}
\newcommand{\nTotalQuestions}{1782}
\newcommand{\nDocuments}{3386}
\newcommand{\vspacea}{-0.3cm}
\newcommand{\vspacesmall}{0cm}
\newcommand{\vspaceb}{-0.5cm}

%\title{A Dataset of Multi-Hop Questions on Historical Parliamentary Debates from the French Third Republic (1870–1940) / Multi-Hop Question Answering for Historical Research: A Dataset Based on Parliamentary Debates from the French Third Republic (1870-1940) / A Multi-Hop QA Dataset from Historical Parliamentary Debates of the French Third Republic (1870–1940)}

\title{HistoriQA-ThirdRepublic : Multi-Hop Question Answering Corpus for Historical Research, Parliamentary Debates from the French Third Republic (1870-1940)}

\name{Aurélien Pellet, Julien Perez, Marie Puren} 

\address{Affiliation1, Affiliation2, Affiliation3 \\
         Address1, Address2, Address3 \\
         author1@xxx.yy, author2@zzz.edu, author3@hhh.com\\
         \{author1, author5, author9\}@abc.org\\}


\abstract{
We present HistoriQA-ThirdRepublic : a French-language dataset of multi-hop historical questions derived from parliamentary debates and newspapers of the French Third Republic. Designed in collaboration with a historian, the corpus captures complex reasoning patterns typical of historical inquiry, including cross-source synthesis, temporal reasoning, and the integration of sparse evidence. The dataset is made of \nTotalQuestions ~\ questions and emphasizes multi-hop connections across heterogeneous historical documents, providing a resource for evaluating retrieval-augmented and large language model systems in domain-specific contexts. We describe the methodology for constructing the corpus, including the selection and alignment of sources, question validation, and metadata integration. While the dataset focuses on French historical documents, our methodology can be readily adapted to other languages and national corpora. Finally, we demonstrate how the corpus can support realistic evaluation scenarios for multi-hop question answering, bridging the gap between NLP benchmarks and the needs of historical scholarship. 
\\ \newline \Keywords{Digital Humanities, Question Answering, Language Representation Models, Information Extraction, Information Retrieval, Evaluation Methodologies, Corpus (Creation, Annotation, etc.)}
%\\ \newline \Keywords{Large Language Models, historical question corpus, multi-hop reasoning, %French Third Republic, French-language dataset, LLM evaluation, retrieval-augmented generation}
}



\begin{document}

\maketitleabstract

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{images/test (1)_cropped.pdf}
  \caption{A sample of multi-hop question generated with Retrieval-Augmented Generation (RAG) pipeline. Given a query about the 1887 military law, the system retrieves relevant documents from heterogeneous sources and generates a multi-hop answer, which can be compared against the ground truth.}
  \label{fig:teaser}
\end{figure*}

\section{Introduction}

Large language models (LLMs) combined with retrieval-augmented generation (RAG) systems \cite{Lewis_Perez_Piktus_Petroni_Karpukhin_Goyal_Küttler_Lewis_Yih_Rocktäschel_et_al._2020} have recently achieved strong results on established benchmarks. A major challenge for LLMs lies in their training data \cite{guo2024biaslargelanguagemodels}. These models are rarely exposed to highly specific and hard-to-access documents, such as historical sources which, when digitized, are often stored in digital libraries. This limitation significantly impacts their ability to process and reason about specialized historical corpora, where domain expertise and contextual understanding are paramount. By grounding generation in retrieved documents, RAG reduces—though does not eliminate—hallucination and, in some cases, enables good performance without task-specific fine-tuning. As RAG architectures diversify \cite{Chang_Jiang_Rakesh_Pan_Yeh_Wang_Hu_Xu_Zheng_Das_et_al._2025,Li_He_Liu_Zhang_Yu_Ye_Zhu_Su_2025}, reported benchmark scores have continued to rise \cite{llama3,gpt4}, but these gains mask important limitations when models are applied to complex, domain-specific tasks.

First, hallucination and retrieval errors remain a problem in long-context and multi-document scenarios: when answers require reasoning over lengthy documents or the synthesis of information from several heterogeneous sources, errors compound, and factual consistency degrades. Even for RAG, it has been shown how failure can increase in long context \cite{Leng_Portes_Havens_Zaharia_Carbin_2024}.

Second, standard benchmarks remain poorly aligned with domain-specific challenges. They usually focus on multiple-choice and short-answer questions, short passages, or single-document question answering \cite{mmlu,Zhuang_Zhang_Cheng_Yang_Liu_Huang_Lin_Rajmohan_Zhang_Zhang_2024}. Recent initiatives have attempted to address this limitation by proposing more targeted benchmarks, such as Humanity’s Last Exam \cite{phan2025humanitysexam}. However, many of these datasets are still relatively general in scope and focus primarily on factual questions.

These shortcomings become particularly evident in the case of historical corpora, where existing benchmarks fail to capture the kinds of reasoning that domain experts require such as  temporal reasoning, cross-source linking, and the handling of highly specific, hard-to-access documents. To illustrate this, we focus on a case study of the French Third Republic (1870–1940), drawing on agenda-setting theory \cite{McCombs_Shaw_1972} to situate parliamentary debates within their media ecosystem. This period, marked by an autonomous Parliament \cite{Morel_2024} and the rise of the “grande presse d’information\footnote{A term coined to describe French major daily newspapers born in the second half of the 19th century.}” \cite{kalifa_trerenty_2011}, provides an ideal context for studying how complex, cross-source questions emerge. Fully digitized corpora of parliamentary debates\footnote{Available online for the years 1880–1940 for both the \href{https://gallica.bnf.fr/ark:/12148/cb328020951/date&rk=150215;2}{Chamber
 of Deputies} and the \href{https://gallica.bnf.fr/ark:/12148/cb34363182v/date&rk=128756;0}{Senate}.}
 exemplify these challenges: historians often need to connect evidence across heterogeneous documents, trace arguments over time, and aggregate sparse mentions dispersed across records. To better capture how issues circulated and gained prominence, we complement this corpus with digitized newspapers from the same period. By linking parliamentary proceedings with contemporary press coverage, we aim to reconstruct the dynamic interplay between institutional discourse and public framing, and to support the generation of questions that reflect the full complexity of historical agenda-setting processes. The issue lies not only in the thematic scope of the questions, but also in the types of reasoning that are relevant to historians. Those requirements affect every component of a RAG pipeline: indexing and chunking methodologies for long texts, retrieval architectures and similarity metrics, the exploitation of metadata and temporal signals, and answer-generation approaches that must support complex argumentative or interpretive responses rather than only fact verification or multiple-choice answers.

%These shortcomings are especially acute for historical corpora such as parliamentary debates from the French Third Republic (1870-1940), which have been fully digitized and are accessible via the digital library of the Bibliothèque nationale de France\footnote{They are fully available online for the years 1880 to 1940 for both the \href{https://gallica.bnf.fr/ark:/12148/cb328020951/date&rk=150215;2}{Chamber of Deputies} and the \href{https://gallica.bnf.fr/ark:/12148/cb34363182v/date&rk=128756;0}{Senate}.}. Historical research routinely requires combining evidence from documents that vary greatly in length and format, that span days, months, or years, and that come from different archives and sources. In the case of studying parliamentary history, a historian may need to relate a newspaper article to a parliamentary speech, trace the evolution of an argument across multiple sittings, or aggregate sparse mentions dispersed across many records. The issue lies not only in the thematic scope of the questions, but also in the types of reasoning that are relevant to historians. Those requirements affect every component of a RAG pipeline: indexing and chunking methodologies for long texts, retrieval architectures and similarity metrics, the exploitation of metadata and temporal signals, and answer-generation approaches that must support complex argumentative or interpretive responses rather than only fact verification or multiple-choice answers.

%In this context, our study contributes to a broader historical research project on how public problems were constructed and placed on the political agenda during the French Third Republic. Drawing on agenda-setting theory \cite{McCombs_Shaw_1972}, which emphasizes the media’s role in shaping public problems and influencing political priorities, we situate parliamentary debates within their media ecosystem. The Third Republic, characterized by an autonomous Parliament and the rise of the grande presse d’information, provides an ideal setting to investigate the circulation of issues between Parliament and the press and to design historically grounded evaluation tasks for RAG.

%Automatic question generation offers a promising means of adapting benchmarks to the specific needs of a given domain. By combining embeddings, metadata, keywords, and clustering techniques, it is possible to generate domain-aligned questions ranging from single-hop factual queries to multi-hop, temporally grounded questions that require cross-source synthesis. Such synthetic question sets can be used to stress different parts of the RAG pipeline and to create more realistic evaluation scenarios for domain specialists \cite{Lin_Chen_Song_Zhang_2024,Li_Zhang_Kong_2025,Hwang_Kim_Lee_2024}.

In this paper, we present a method for generating, analyzing, and validating high-quality questions that reflect historians’ research problems. Drawing on historical parliamentary debates (sources that are thematically rich, extensive, and span several decades) and cross-referencing them with contemporaneous press archives, we demonstrate how to incorporate temporal dynamics and source sparsity into question generation, and how clustering and metadata can help identify promising multi-hop candidates. We further analyze the structure of both the corpora and the questions to identify natural seeds for multi-hop question generation. We evaluate RAG systems at both the retrieval and generation stages on a carefully selected subcorpus of French Third Republic parliamentary proceedings.

Our work makes four main contributions. First, we propose a dataset\footnote{Dataset available at \href{https://anonymous.4open.science/r/debattre-generate-questions-anon-9325}{Anonymous GitHub}. The dataset will be publicly released upon acceptance of the paper.} historically aligned, requiring reasoning across diverse sources and formats, and designed to challenge RAG systems. It comprises \nSHqueries ~\ single-hop and \nMHqueries ~\ multi-hop questions, all automatically generated through controlled prompting and validated by a historian for factual and contextual coherence. Second, we introduce a method for generating domain-specific multi-hop questions from heterogeneous corpora, combining embedding-based similarity, temporal constraints, and expert-guided prompt engineering. We further evaluate the resulting questions by analyzing their evolution in semantic space and conducting a preliminary classification of their types. Third, we introduce an evaluation protocol that leverages historian judgments to assess both question and answer quality. Finally, we conduct an automatic evaluation of RAG retrieval and generation performance and investigate the alignment between LLM-based judgments and human annotations.

Taken together, these contributions aim to bridge the gap between off-the-shelf RAG benchmarks and the specific needs of historical scholarship, enabling more meaningful evaluation and targeted development of retrieval and generation methods for complex, domain-specific corpora in the humanities and social sciences.



\section{Related work}

%The computational study of debates from the Third French Republic has already yielded significant insights. Methods such as topic modeling using LDA \cite{bourgeois:hal-03526254} or dense semantic embeddings like BERT \cite{puren:hal-04128262} have offered a broader perspective on the range of issues addressed in the French parliament.

%Automatic question generation has already shown promising results in this context. Recent studies have assessed the ability of retrievers to reason over long-context documents, focusing in particular on the comparison of chunking strategies and the evaluation of different retriever architectures \cite{pellet:hal-04832663,pellet:hal-05193494}. Earlier work relied on fixed-size chunks \cite{Smith_Troynikov_2024}, determined by the chosen segmentation strategy, but did not account for the intrinsic structure. As a result, some chunks may naturally be longer than others, and enforcing uniform segmentation can be problematic.

\paragraph{Multi-Document Reasoning Datasets}
Multi-document reasoning have been developed to challenge information retrieval and question answering systems. Resources such as HotpotQA~\cite{hotpotqa} and 2WikiMultihopQA~\cite{ho-etal-2020-constructing} provide deeper insights into the question-generation process. In particular, HotpotQA introduces a detailed taxonomy of question categories, covering various question types and forms of multi-hop reasoning. Building on this line of work, datasets like MoreHopQA~\cite{morehopqamultihopreasoning} propose even more complex multi-hop questions that require additional layers of reasoning, including commonsense, arithmetic, and symbolic inference.

\paragraph{Automatic Question Generation for RAG Evaluation}
Research on automatic question generation has shown promising results for improving the evaluation of RAG systems in specific context, particularly by enabling the systematic assessment of a model’s reasoning capabilities over long and structured documents. By combining embeddings, metadata, keywords, and clustering techniques, it is possible to generate domain-aligned questions ranging from single-hop factual queries to multi-hop, temporally grounded questions that require cross-source synthesis. Such synthetic question sets can be used to stress different parts of the RAG pipeline and to create more realistic evaluation scenarios for domain specialists \cite{Lin_Chen_Song_Zhang_2024,Li_Zhang_Kong_2025,Hwang_Kim_Lee_2024}.

\paragraph{Retriever Performance in Long-Context and Historical Documents}
Recent studies have examined the ability of retrievers to reason over long-context documents in a historical context with particular attention to the comparison of chunking strategies and the evaluation of different retriever architectures~\cite{pellet:hal-04832663,pellet:hal-05193494}. Earlier work often relied on fixed-size chunks~\cite{Smith_Troynikov_2024}, determined by the chosen segmentation strategy, without accounting for the documents’ intrinsic structure. As a result, some chunks may naturally be longer than others, and enforcing uniform segmentation can be problematic.

%When dealing with parliamentary debates, a natural chunking strategy (such as splitting by speech turns) can result in chunks of highly variable length. A retriever returning $k$ documents for a given query may thus encounter substantial variability in input size: for one query, the retrieved passages may total 10,000 tokens, while for another, the same $k$ results may contain only 2,000 tokens. In addition, documents from newspaper corpora tend to be shorter on average, as shown in Table~\ref{tab:corpus-stats}. This discrepancy reduces the comparability of results across different $k$ values. To address this limitation, a refinement has been introduced: computing standard metrics such as recall@$k$—the proportion of times the gold document is retrieved among $k$ candidates—at the token level, resulting in a recall@Tokens metric calculated for various token thresholds.

%However, existing experiments have primarily focused on chunking strategies, leaving the issue of query generation largely unexplored. Only single-hop questions were generated, and evaluation efforts targeted exclusively the retriever component of RAG systems. Furthermore, question generation relied solely on debates from the French Third Republic, thereby restricting the LLM’s reasoning capabilities to a single type of source.

%\subsection{Historical Context}
%TO DO

\section{Corpus Description}

%\paragraph{Historical Context.} This study contributes to a historical research project that investigates how public problems were constructed and placed on the political agenda during the French Third Republic. Agenda-setting theory posits that the media do not merely transmit information but actively shape public problems by directing attention to specific issues, thereby influencing the political agenda. Since its formulation by McCombs and Shaw in 1972\cite{McCombs_Shaw_1972}, it has been widely used to analyze the relationship between media, public opinion, and political decision-making. The French Third Republic provides a particularly valuable case for studying agenda-setting, as it combined a highly autonomous Parliament with the rise of a powerful press. During this period, MPs held extensive powers in defining government policy and enjoyed significant freedom in selecting debate topics, making the parliamentary agenda a central arena of political life. At the same time, the emergence of the ``\textit{grande presse d’information}\footnote{Expression coined to describe French major daily newspapers born in the second half of the 19th century.}'' in the late 19th century established the press as both a widely read medium and an active creator of public opinion, reflecting and shaping major political trends. This dual dynamic—an empowered legislature and an influential press—makes the Third Republic an ideal setting to examine the circulation of issues between Parliament and the media.

%The parliamentary debates of the French Third Republic are accessible through Gallica in OCR format.  The complete collection represents more than 11,000 scanned images that have been automatically transcribed. Given the complexity and size of this corpus, we decided to restrict our analysis to a single year, 1887. This year offers continuous debates across all sessions of parliament, while remaining sufficiently bounded to allow for a fine-grained exploration of our methods.  To evaluate the capacity of RAG systems to produce relevant multi-hop questions, we complemented the debates with two contemporary newspapers, namely \textit{Le Gaulois} and \textit{L’Intransigeant}.\\

The parliamentary debates of the French Third Republic are available as plain text files resulting from OCR, provided through the digital library of the Bibliothèque nationale de France. For this study, we focus on the 1887 debates in the Chamber of deputies, which provides continuous coverage of parliamentary sessions while remaining small enough for detailed analysis\footnote{For the Chamber of Deputies only, there are approximately 11,000 automatically transcribed pages for the 1885–1889 parliamentary term.}. In 1887, the publicity of parliamentary debates was also firmly established, allowing the press to report extensively on parliamentary proceedings. That same period was marked by international tensions (most notably with Germany), political scandals reaching the highest levels of the state, and a surge of anti-parliamentarism crystallized in the ``Boulangist crisis'', which posed a major challenge to the republican regime\footnote{General Boulanger, whose popularity continued to grow during his two terms as Minister of War in 1886 and 1887, subsequently assumed the leadership of a vigorous anti-parliamentary coalition.}. These events found in the press both a powerful amplifier and, in some cases, an active ally, as certain newspapers openly supported Boulanger’s followers. These crisis illustrate how parliamentary debates and press coverage became deeply intertwined, shaping public perception and fueling political polarization. This context provides a particularly rich case for studying the interplay between Parliament and media, as it captures both institutional discourse and its mediation in the public sphere.

We complement the corpus of parliamentary proceedings with two contemporary newspapers, \textit{Le Gaulois} and \textit{L’Intransigeant}, using plain-text transcripts released by the Bibliothèque nationale de France\footnote{Datasets are available via the \href{https://api.bnf.fr/index.php/fr/texte-des-documents-de-presse-du-projet-europeana-newspapers-xixe-xxe-siecles}{API and dataset portal} of the Bibliothèque nationale de France.}. These daily newspapers represent two opposing positions on the political spectrum: on the one hand, monarchist and conservative right-wing views; on the other, socialism, although \textit{L’Intransigeant} would gradually shift toward populism. This multimodal perspective is designed to test whether RAG can connect parliamentary discourse with press coverage, thus requiring reasoning across heterogeneous sources.

While this temporal focus inevitably narrows the scope of the corpus, it remains substantial enough to enable close collaboration with historians, who can provide qualitative feedback on generated questions and RAG outputs.

\begin{table*}[t]
\centering
\begin{tabular}{lcccc}
\toprule
 & \multicolumn{2}{c}{\textbf{Les Débats}} & \textbf{Le Gaulois} & \textbf{L’Intransigeant} \\
\cmidrule(lr){2-3}
 & {All documents} & {Filtered} &  &  \\
\midrule
Number of documents        & 3229 & 963 & 78  & 79 \\
Average tokens / document  & 2020 & 4433 & 922 & 376 \\
Median tokens / document   & 279 & 716 & 802 & 317 \\
Range (min–max)            & 1–63121 & 143–62998 & 51–4023 & 6–1335 \\
Standard deviation          & 6636 & 9697 & 675 & 257 \\
\bottomrule
\end{tabular}
\caption{Statistics of the corpus: \textit{Les Débats}, \textit{Le Gaulois}, and \textit{L’Intransigeant}. 
For \textit{Les Débats}, we report both the full dataset and a filtered subset excluding non-debate sections.}
\label{tab:corpus-stats}
\end{table*}


Table~\ref{tab:corpus-stats} reports descriptive statistics for the three corpora used in our study.
Each corpus consists of documents or chunks. For the parliamentary debates, segmentation relied on regular expressions identifying session boundaries and speaker turns, while enforcing a minimum chunk size of 10,000 characters. Newspaper articles were subsequently extracted via dedicated regex patterns applied to the relevant portions of the reports. The debates dataset (Les Débats) comprises more than 3,200 documents.

To enhance textual coherence, we filtered out non-debate sections (e.g., agendas, procedural notes, vote lists). The resulting 963 documents display wide variation in length, mirroring the thematic diversity and rhetorical dynamics of the assembly. This filtered subset thus offers a more faithful view of deliberative discourse while reducing noise for downstream analysis.

The two complementary corpora consist of press reports on parliamentary sessions published in \textit{Le Gaulois} and \textit{L’Intransigeant}\footnote{These reports, written by specialized journalists, summarized, sometimes in the form of verbatim transcripts, and commented on the debates held in the Chamber. For readers, they were often the primary source of information on parliamentary activity.}.

Although shorter than the parliamentary proceedings, these texts add interpretative and often polemical perspectives. Combined with the debates, they create a rich contrast between institutional discourse and mediated public interpretation—an essential feature for evaluating multi-hop reasoning.


\section{Methodology}


%In this section, we describe our approach for generating and evaluating historical questions from our corpora. We distinguish between single-hop questions, which rely on a single text segment, and multi-hop questions, which require linking information across debates and newspapers. For each case, we detail the prompting strategies, embedding-based similarity measures, and visualization techniques used to assess the relation between generated questions and their source material.

In this section, we present our approach to generating and evaluating historical questions from the corpus. We describe the prompting strategies, embedding-based similarity measures, and visualization techniques used to assess the relation between the generated questions and their source material.


\subsection{Single-hop Question Generation}
\begin{comment}
We begin by focusing on the generation of questions answerable from a single chunk of text. For this experiment, we restricted the scope to parliamentary debates. As described above, we used the filtered sub-corpus, yielding 963 documents for analysis.

Several iterations were conducted in collaboration with the historian on our team to refine the prompting strategy. The final version included clearer instructions and selected few-shot examples illustrating the features we sought in the generated questions. Specifically, we aimed for questions that (i) target the central topic of the debate rather than peripheral procedural details, (ii) emphasize political positions and key historical issues, and (iii) avoid verbatim reuse of the source text. The model was required to produce a structured JSON output, with the possibility of returning an empty list when no relevant question could be generated.

To visualize the semantic relationship between generated questions and their source texts, we computed TF--IDF representations and applied similarity matching between question embeddings and text chunks. We then projected these embeddings into two dimensions using PCA, as shown in Figure~\ref{fig:pca_embeddings_questions_simplified}. 

On average, the cosine distance between questions and their source chunks is \textbf{1.04} for the baseline prompt and \textbf{1.26} for the optimized prompt, indicating a greater semantic divergence in the latter case. Questions produced with the optimized prompt therefore tend to lie farther from the centroid of the source-text cluster compared to those generated by the baseline model. While both sets remain thematically close to the original material, this increased distance suggests that optimized questions are less likely to reproduce text verbatim. Instead, they exhibit more paraphrasing and conceptual reformulation—a trend confirmed through qualitative inspection.
\end{comment}

We focus on generating questions answerable from single text chunks, restricting our scope to parliamentary debates from the filtered sub-corpus (963 documents). Through iterative refinement with our team historian, we developed a prompt designed to favour (i) substantive debate content over procedural detail, (ii) the articulation of political positions and key issues, and (iii) paraphrase over surface-level text reuse. The model returned structured JSON output, yielding an empty list when no suitable question could be produced.

To assess semantic relationships, we computed TF–IDF representations and applied PCA dimensionality reduction (Figure~\ref{fig:pca_embeddings_questions_simplified}). Average cosine distances between questions and their source chunks were 1.04 (baseline) and 1.26 (optimized), indicating higher semantic divergence after prompt refinement. This increase suggests that the optimized prompt encourages conceptual reformulation rather than lexical copying—a trend corroborated by qualitative inspection.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{images/output.png}
\vspace{\vspaceb}
\caption{TF–IDF projection of generated questions and source texts. Questions from the optimized prompt lie farther from their source chunks than those from the base prompt, indicating less lexical overlap and greater complexity.}
\label{fig:pca_embeddings_questions_simplified}
\vspace{\vspacea}
\end{figure}

\begin{comment}
\begin{table}[htbp]
\centering
\tiny
%\renewcommand{\arraystretch}{1.3}
\begin{tabular}{p{0.8cm}p{2.8cm}p{2.8cm}}
\toprule
\textbf{Date} & \textbf{Question (French)} & \textbf{English Translation} \\
\midrule
1887-05-23 & Quels sont les principaux points de désaccord entre le ministre des travaux publics, M. de Hérédia, et M. Jaurès concernant le projet de loi sur la sécurité des mineurs et la représentation des ouvriers dans les mines en 1887 ? & What are the main points of disagreement between the Minister of Public Works, M. de Hérédia, and M. Jaurès regarding the bill on miner safety and worker representation in mines in 1887? \\[1em]
\midrule
1887-11-24 & Quels projets de loi ont été adoptés en 1887 concernant les emprunts des villes de Sedan, Dijon et Niort, et quelle en est la raison ? & What bills were adopted in 1887 concerning the loans of the cities of Sedan, Dijon and Niort, and what was the reason? \\[1em]
\midrule
1887-10-29 & En 1887, quelle est la position de la Chambre des députés sur la demande d'urgence pour la loi concernant le traitement des instituteurs ? & In 1887, what is the position of the Chamber of Deputies on the urgent request for the law concerning teachers' salaries? \\[1em]
\bottomrule
\end{tabular}
\caption{Sample of single-hop questions generated from the corpus. French questions are paired with their English translations.}
\label{tab:singlehop_examples}
\end{table}
Table~\ref{tab:singlehop_examples} shows examples of single hop questions generated.
\end{comment}

\subsection{Multi-hop question generation}

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{images/projection_newspaper_to_debat_similarity_v2_cut.png}
\vspace{\vspaceb}
\caption{UMAP visualization of semantic relationships between press and debate articles. Each main point (black-bordered) represents an article from \textit{Le Gaulois} (blue) or \textit{L’Intransigeant} (red). Lines connect each article to its three most similar debate chunks (Cohere embeddings, cosine similarity). The 2D projection reveals cross-newspaper thematic links between press and parliamentary discourse.}
\label{fig:newspaper_cross_references}
\vspace{\vspacea}
\end{figure}


We next address the generation of multi-hop questions that span both parliamentary debates and newspaper articles. A natural starting point is to pair text chunks that are temporally proximate. While this approach substantially reduces the number of potential combinations, it does not ensure that the retrieved documents are semantically related, and thus suitable for multi-hop reasoning within a RAG framework.

Figure~\ref{fig:newspaper_cross_references} presents a two-dimensional projection of newspaper samples, each linked to its top-3 most similar debate chunks, highlighted in yellow. This visualization illustrates the potential for identifying candidates for multi-hop question generation: in many cases, clusters of semantically close text segments can be observed in the embedding space. The embedding model appears to capture meaningful relationships between press samples and parliamentary debates. This unsupervised visualization supports our assumption that appropriate candidates for multi-hop question generation can be detected automatically. To refine the selection, we applied a similarity threshold of $\tval$, ensuring that only highly relevant pairs were retained.
For the multi-hop question generation phase, we once again collaborated with the historian on our team to iteratively refine the prompting strategy. Our expert provided domain-specific examples that helped define the types of relationships and reasoning chains most valuable for historical interpretation.

We identified two complementary dimensions for guiding the generation process. The first concerns the \textit{type} of questions.
Inspired by the HotpotQA taxonomy of questions~\cite{hotpotqa}, early iterations produced a first set of multi-hop questions that involved, for instance, identifying an opinion expressed in the debates and examining how the press reacted to it; we refer to these as \textit{follow-up questions}. We then extended this to \textit{bridge-entity questions}, where a shared reference (e.g., a political figure or event) connects a debate passage with a newspaper article. Finally, we introduced \textit{comparative questions} designed to contrast differing viewpoints across multiple sources. The full prompts used for these experiments are provided in the supplementary material available in the \href{https://anonymous.4open.science/r/debattre-generate-questions-7B3D}{Anonymous GitHub} repository.

The second dimension relates to the \textit{source pairing strategy}. We focused on two types of document relationships:\\

\noindent\textbf{Newspaper-to-debate questions.} For each newspaper chunk, we retrieved its top-1 most similar debate passage based on cosine similarity between embeddings. This direction (press to debate) was chosen to limit the total number of generated questions while maximizing the diversity of reasoning types. A similarity threshold of $\tval$ was again applied to filter weakly related candidates.\\

\noindent\textbf{Newspaper-to-newspaper questions.} We computed all similarity pairs between the two selected newspapers. A threshold of $\tval$ was applied, along with a temporal constraint requiring that the paired articles be published no more than one week apart.

For the present study, we set aside multi-hop questions relying exclusively on parliamentary debates. Although such questions could provide insights into the evolution of political arguments over time, they carry the risk that multi-hop reasoning would occur across adjacent or even identical sessions, thereby limiting the diversity of perspectives. By combining debates with newspapers, we instead emphasize the interpretive gap between official parliamentary proceedings and their representation in the press. This approach introduces an additional challenge: the retrieval process must operate across distinct corpora, each characterized by different styles, formats, and lengths. 

Table~\ref{tab:multihop_examples} presents samples of the generated questions. In total, we produced $\nMHqueries$ multi-hop questions. Figure~\ref{fig:comparison} presents a sunburst plot for the first four words of the generated questions.

\begin{table}[htbp]
\centering
\tiny
\begin{tabular}{p{0.4cm}p{2cm}p{2cm}p{2cm}}
\toprule
\textbf{Date} & \textbf{Question (French)} & \textbf{English Translation} & \textbf{Type / Sources} \\
\midrule
1887 & En 1887, quelle est la position de L'Intransigeant et de Le Gaulois concernant l'intégration des individus nés en France de parents étrangers dans le service militaire, et comment ces positions reflètent-elles leurs visions respectives de la nationalité et de l'armée ? & In 1887, what is the position of L'Intransigeant and Le Gaulois regarding the integration of individuals born in France to foreign parents into military service, and how do these positions reflect their respective views on nationality and the army? & Follow-up / Le Gaulois + L'Intransigeant \\[1em]
\midrule
1887 & En 1887, comment la presse critique-t-elle le processus de création de chaires d'enseignement supérieur, et en quoi cela contraste-t-il avec les arguments du ministre de l'Instruction publique lors du débat parlementaire ? & In 1887, how does the press criticize the process of creating university chairs, and how does this contrast with the arguments of the Minister of Public Instruction during the parliamentary debate? & Follow-up / Le Gaulois + Les Débats \\[1em]
\midrule
1887 & Quelle tonalité la presse adopte-t-elle à propos du député qui a défendu une augmentation des crédits pour l'agriculture face à la concurrence étrangère ? & What tone does the press adopt regarding the deputy who defended an increase in credits for agriculture in the face of foreign competition? & Bridge-entity / Les Débats + L'Intransigeant \\[1em]
\bottomrule
\end{tabular}
\caption{Sample of generated multi-hop questions from the corpus. French questions are paired with their English translations, and the last column indicates the question type and associated sources.}
\label{tab:multihop_examples}
\vspace{\vspacea}
\end{table}

\begin{figure*}[htbp]
\centering

\begin{subfigure}[t]{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/generic.png}
    \caption{Follow-up questions.}
    \label{fig:first_figure}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/comparative.png}
    \caption{Comparative questions}
    \label{fig:second_figure}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/bridgeentity.png}
    \caption{Bridge-entity questions}
    \label{fig:third_figure}
\end{subfigure}

\caption{Details of the generated questions, illustrating frequency patterns for each category. Each segment represents a possible word continuation in the sequence of question prefixes (lemmatized and in French). Empty colored blocks indicate rare suffixes not displayed individually. Subfigure~(a) corresponds to follow-up questions, (b) to comparative questions, and (c) to bridge-entity questions.}
\label{fig:comparison}
\end{figure*}



\section{Experiments}

%\subsection{Research Questions}

This section addresses three main questions: 
How do current models perform on a long-context historical dataset ? (RQ1)
To what extent do LLM-based evaluations of retrieval and generation align with human judgments ? (RQ2)
How are question types related to model performance and error patterns ? (RQ3)

%We analyze the relationship between the linguistic and informational structure of questions and the observed model outputs, identifying patterns that contribute to success or systematic errors.


%\begin{enumerate}
% \item \textbf{How can relevant candidate documents be effectively identified across heterogeneous historical sources, especially when dealing with long documents, for single- and multi-hop question generation?}

%  \textit{Operationalization \& evidence:} expert-driven identification by a domain historian combined with corpus sectioning and text-parsing heuristics to extract candidate chunks (temporal filtering, structural cues, and rule-based/statistical parsing); qualitative inspection illustrated in Table~\ref{tab:corpus-stats} and~\ref{fig:newspaper_cross_references}.
 % \vspace{0.4em}


%  \item \textbf{How can we generate historically meaningful questions that avoid verbatim reproduction of sources?}  
%  \textit{Operationalization \& evidence:} an iterative, historian-in-the-loop prompting workflow: initial prompts (baseline) are refined through structured historian feedback on produced questions, selection of few-shot examples, and prompt ablations to improve clarity and historical relevance; effectiveness is measured via semantic-distance analyses and manual historian validation (Figure~\ref{fig:pca_embeddings_questions_simplified}, Table~\ref{tab:singlehop_examples}).
%  \vspace{0.4em}

% \item \textbf{Comment les modèles actuels performent sur a long context historical dataset}  

  %  \item \textbf{In order to evaluate retrieval and generation to what extent do LLM-based evaluations align with human judgments?}  
  %\textit{Operationalization \& evidence:} retrieval metrics (Recall@K, Accuracy@K, MRR@K in Table~\ref{tab:retrieval_results_improved}), LLM-as-a-judge vs.\ human alignment (Tables~\ref{tab:alignement_judges},~\ref{tab:confusion_judges}) and RAG answer performance (Table~\ref{tab:llm_eval_clean}).

 %   \item Comment la structure des questions est liée à la performance et les erreurs des modèles

%\end{enumerate}

\vspace{\vspacesmall}

\subsection{Experimental Settings}

Question generation experiments were conducted using the Cohere API. We employed the \texttt{Command-R} model for reasoning-oriented tasks and \texttt{Command-A} for both question generation and RAG-based answering. Dense retrieval was performed using embeddings computed with \texttt{cohere-embed-v4}, while retrieved passages were reranked with \texttt{cohere-rerank-multilingual-v3.0}. The vector database was implemented using ChromaDB. 

In addition to Cohere models, we evaluated several large language models for both generation and RAG-based answering, including \texttt{Llama~3.2~3B~Instruct}, \texttt{Llama~3.3~70B~Instruct}, \texttt{GPT-OSS~20B}, and \texttt{GPT-4o-mini}.

The final prompts and system configuration were refined through multiple iterative cycles in close collaboration with our team historian, who is a specialist of this period. This process enabled the selection of representative few-shot examples for question generation, LLM-as-a-judge evaluation, and RAG-based answering.

\begin{comment}
\subsection{Single-Hop vs Multi-Hop Question Classification}

We employed a logistic regression classifier to distinguish between single-hop and multi-hop questions, using Cohere embeddings as latent representations. The model achieved high performance on this binary classification task. 

\begin{table}[htbp]
\centering
\footnotesize
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{cc|cc}
\toprule
& & \multicolumn{2}{c}{\textbf{Predicted}} \\
& & \textbf{Single-Hop} & \textbf{Multi-Hop} \\
\midrule
\multirow{2}{*}{\rotatebox{90}{\textbf{Actual}}}
& \textbf{Single-Hop} & 494 & 1 \\
& \textbf{Multi-Hop}  & 5   & 265 \\
\bottomrule
\end{tabular}
\caption{
Confusion matrix for the \textbf{Single-Hop vs. Multi-Hop} classifier.  
Rows correspond to \textbf{actual (human) labels}, and columns to \textbf{model predictions}.
}
\label{tab:confusion_matrix_singlehop}
\end{table}

Table~\ref{tab:confusion_matrix_singlehop} shows that this simple classifier achieves near-perfect performance, with a test accuracy of 99.22% and minimal signs of overfitting. The confusion matrix reports only 6 misclassifications out of 765 test samples: 5 multi-hop questions incorrectly classified as single-hop and 1 single-hop question misclassified as multi-hop.
\end{comment}

\begin{table*}[ht]
\centering
\footnotesize
\begin{tabular}{lcc}
\toprule
\multicolumn{1}{c}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{Accuracy}} \\
\cmidrule(lr){1-1} \cmidrule(lr){2-3}
\textbf{Model} & \textbf{Single-hop} & \textbf{Multi-hop} \\
\midrule
\texttt{Llama-3.3-3b-Instruct} & 24.10 & 3.2 \\
\texttt{Llama-3.3-70b-Instruct} & 53.53 & 17.92 \\
\midrule
\texttt{gpt-oss-20b} & \textbf{58.85} & \textbf{18.86} \\
\texttt{gpt-4o-mini} & 52.72 & 13.77 \\
\bottomrule
\end{tabular}
\caption{Evaluation on Single-hop and Multi-hop datasets, using accuracy as judged by an LLM evaluator. Larger models show higher performance across both datasets, especially on the Multi-hop benchmark.}
\label{tab:llm_eval_clean}
\end{table*}



\subsection{Retrieval Evaluation}

We evaluate the retrieval stage of the RAG pipeline by measuring how often the gold source chunks used to generate each question are successfully returned by the retriever. Because questions were generated from known source passages, this setup provides an unambiguous oracle for retrieval performance. Table~\ref{tab:retrieval_results_improved} reports Recall@K, Accuracy@K (i.e the proportion of questions for which \emph{all} gold documents appear in the top-$K$), and MRR@K for the main experimental configurations.

Single-hop retrieval is highly effective: the embedding retriever alone achieves Recall@3 = 67.8, and reranking provides modest gains. This indicates that for questions relying on a single chunk, dense embeddings capture most of the necessary signal.

Multi-hop retrieval is more challenging. Cross-newspaper retrieval achieves low recall with embeddings alone (Recall@3 = 14.3). Cross-domain retrieval (newspaper–debate) performs better (embedding Recall@3 = 47.8). In general cross-domain retrieval performs way better than cross-newspaper retrieval. This indicates strong corpus-bias effect when retrieval is performed on the full database (debates + newspapers): the retriever frequently returns debate chunks even for newspaper–newspaper queries. Concretely, for $K=3$ an average 80.6\% of retrieved documents belonged to the parliamentary debates corpus; this proportion increases with $K$ (83.6\% for $K=5$, 86.8\% for $K=10$). Restricting retrieval to the newspaper-only sub-dataset markedly improves cross-newspaper performance, confirming that source-type priors and corpus-aware retrieval are crucial for cross-source tasks.

A non-obvious finding is that the effect of reranking is \emph{not uniformly positive}. While reranking often improves Recall@10 and MRR@10, it can \emph{reduce} top-$K$ metrics and Accuracy@3 in some configurations. For example, in the newspaper–debate setting embedding-only Recall@3 = 47.8 drops to 45.5 after reranking This behavior likely stems from mismatches between reranker training objectives, domain differences, and corpus biases.


%In summary, high single-hop accuracy suggests that answer synthesis, rather than retrieval, is the main bottleneck for simple questions. In contrast, low multi-hop recall and the occasional negative impact of reranking are key limiting factors for end-to-end RAG performance.



\begin{table*}[ht]
\centering
\scriptsize
\setlength{\tabcolsep}{3pt}
\begin{tabular}{l c c c l c c c c c c c c c}
\toprule
\textbf{Category} & \textbf{Queries} & \textbf{Quest. Types} & \textbf{Sources} & \textbf{Method} &
\multicolumn{3}{c}{\textbf{Recall}} &
\multicolumn{3}{c}{\textbf{Accuracy}} &
\multicolumn{3}{c}{\textbf{MRR}} \\
\cmidrule(lr){6-8} \cmidrule(lr){9-11} \cmidrule(lr){12-14}
& & & & & @3 & @5 & @10 & @3 & @5 & @10 & @3 & @5 & @10 \\
\midrule

%-------------------- SINGLE-HOP --------------------%
\rowcolor{gray!10}
\multicolumn{14}{l}{\textbf{Single-hop retrieval}} \\
\midrule
Single-hop (Intra-source) & \nSHqueries & -- & -- & Embedding & 67.8 & 71.6 & 76.4 & 67.8 & 71.6 & 76.4 & 60.6 & 61.5 & 62.1 \\[2pt]
 &  & & & Reranking & 70.6 & 73.1 & 76.4 & 70.6 & 73.1 & 76.4 & 63.1 & 63.7 & 64.1 \\

%-------------------- MULTI-HOP --------------------%
\rowcolor{gray!10}
\multicolumn{14}{l}{\textbf{Multi-hop retrieval}} \\
\midrule

Cross-newspaper Retrieval & 314 & F, C & Gau., Int., Deb. & Embedding & 14.3 & 19.9 & 29.3 & 1.3 & 4.8 & 12.1 & 10.3 & 11.6 & 12.8 \\
& & & & Reranking & 10.4 & 19.0 & 48.4 & 3.5 & 7.0 & 25.2 & 6.2 & 8.1 & 11.9 \\[2pt]

\cmidrule(l){1-14}

Newspaper–Debate Retrieval & 571 & F, C, B & Gau., Int., Deb. & Embedding & 47.8 & 57.4 & 68.6 & 23.3 & 36.3 & 51.5 & 34.6 & 36.8 & 38.3 \\
& & & & Reranking & 45.5 & 55.3 & 74.3 & 17.5 & 28.2 & 57.4 & 35.0 & 37.2 & 40.0 \\[2pt]

\cmidrule(l){1-14}

All Multi-hop (Aggregated) & \nMHqueries & F, C, B & Gau., Int., Deb. & Embedding & 35.9 & 44.1 & 54.6 & 15.5 & 25.1 & 37.5 & 26.0 & 27.8 & 29.2 \\
& & & & Reranking & 33.0 & 42.4 & 65.1 & 12.5 & 20.6 & 56.0 & 24.8 & 27.0 & 29.0 \\[2pt]

%-------------------- ALL --------------------%
\rowcolor{gray!10}
\multicolumn{14}{l}{\textbf{Overall results}} \\
\midrule
All Questions (Single + Multi-hop) & \nTotalQuestions & -- & -- & Embedding & 52.0 & 57.9 & 65.6 & 41.8 & 48.5 & 57.1 & 43.4 & 44.8 & 45.8\\
& & &  & Reranking & 51.9 & 57.9 & 70.8 & 41.8 & 47.0 & 61.3 & 44.1 & 45.4 & 47.1\\[2pt]

\bottomrule
\end{tabular}
\caption{Retrieval performance across query types using \texttt{embed-V4.0} embeddings and \texttt{cohere-rerank-multilingual-v3.0}. Single-hop queries retrieve from one source; multi-hop queries span multiple sources (cross-newspaper within press, or cross-domain between press and debates). Results shown for embedding-based and reranking methods.}
\begin{flushleft}
\footnotesize
Question types: \textbf{F} = Follow-up, \textbf{C} = Comparative, \textbf{B} = Bridge entity.  
Sources: \textbf{Gau.} = \textit{Le Gaulois}, \textbf{Int.} = \textit{L’Intransigeant}, \textbf{Deb.} = \textit{Les Débats parlementaires}.
\end{flushleft}
\label{tab:retrieval_results_improved}
\end{table*}


\subsection{LLM-as-a-Judge Evaluation}

In this section, we evaluate the alignment between human judgments and LLM-as-a-judge evaluations. A subset of $50$ questions, together with their RAG-generated answers, was annotated by our domain expert. Our objective is to assess how closely an LLM-as-a-judge can replicate these human 
evaluations. 

We made multiple iterations over the questions to ensure their relevance and to check for potential hallucinations. RAG answers were carefully read against the source document and annotated as correct, incorrect, or partial. We report the \textit{alignment rate}, defined as the percentage of cases in which the human and the LLM agree. We consider two judging modes:\\

\noindent\textbf{Answer-reference mode :} in which the LLM-as-a-judge is given the question, the RAG-generated answer (based on retrieved documents), and the reference answer produced during the question-generation phase. The LLM is asked to assess whether the RAG answer aligns with the reference answer.\\

\noindent\textbf{Source-oracle mode :} in which the LLM-as-a-judge is given the question, the RAG-generated answer, and the gold documents used to generate the question. The LLM is asked to determine whether the RAG answer is consistent with these source documents.

The first mode requires an intermediate step to generate a reference answer, whereas the second relies directly on the gold documents. We compare both the alignment between human and LLM judgments, and the agreement between the two LLM judging modes.

We experimented with different prompt configurations for the LLM judge, using two models: a base model (\texttt{Command-A}) and a reasoning-enhanced variant. For prompts, we compared (i) a standard formulation allowing partial answers and (ii) a constrained formulation with only two labels (correct or incorrect), in which partially correct human annotations were treated as incorrect. We also evaluated alignment in cases where not all gold documents were successfully retrieved.

The detailed alignment analysis is provided in Table~\ref{tab:confusion_judges}. In the \textit{Source-oracle} setting—where the judge has access to the original chunks and the corresponding LLM-generated answers—the alignment is perfect for instances where RAG outputs are deemed correct by human evaluators. Across the 20 questions labeled as correct by humans, the LLM judge reached full agreement. Conversely, for incorrect answers, the RAG system exhibits markedly lower alignment.

When using a high-capacity model such as Cohere Command-R, the LLM judge appears highly reliable for answers it classifies as correct. Restricting the analysis to RAG systems using such high-quality models further strengthens this observation.After filtering the dataset to retain only instances where RAG answers were validated as correct, 530 questions remain, distributed across the \textit{Follow-up} (300), \textit{Comparative} (121), and \textit{Bridge Entity} (109) categories.

\begin{table*}[ht]
\centering
\small
\setlength{\tabcolsep}{4pt}
\begin{tabular}{
  lccc@{\hskip 1.8em}lccc
}
\toprule
\multicolumn{4}{c}{\textbf{Answer-reference}} &
\multicolumn{4}{c}{\textbf{Source-oracle}} \\
\cmidrule(lr){1-4} \cmidrule(lr){5-8}
& \multicolumn{3}{c}{\textbf{LLM Annotation}} &
& \multicolumn{3}{c}{\textbf{LLM Annotation}} \\
\cmidrule(lr){2-4} \cmidrule(lr){6-8}
\textbf{Human Annotation} & \textbf{Correct} & \textbf{Incorrect} & \textbf{Total} &
\textbf{Human Annotation} & \textbf{Correct} & \textbf{Incorrect} & \textbf{Total} \\
\midrule
Correct   & \textbf{10} & 10 & 20 & Correct   & \textbf{20} & 0  & 20 \\
Incorrect & 6  & \textbf{24} & 30 & Incorrect & 16 & \textbf{14} & 30 \\
\midrule
\textbf{Total} & 16 & 34 & 50 & \textbf{Total} & 36 & 14 & 50 \\
\bottomrule
\end{tabular}
\caption{
Confusion matrices comparing LLM-based judges with human annotations. Two settings are considered: (1) \textit{Answer-reference}, which assesses answers against a reference answer, and (2) \textit{Source-oracle}, which evaluates answers based on the source documents. %Rows correspond to \textbf{human annotations}, while columns correspond to \textbf{LLM annotations}.  
}
\label{tab:confusion_judges}
\end{table*}

\subsection{RAG Answer Evaluation}

After filtering for questions where the RAG answers were correctly evaluated by the LLM-as-a-Judge, we now perform a full evaluation of the RAG pipeline on the filtered dataset. In this setup, we do not operate in oracle mode, allowing the retriever to potentially return incorrect documents. We use a configuration with $K=3$ retrieved documents and no reranking, as reranking was found to negatively affect performance on the multi-hop question dataset. The RAG system is prompted to answer each question based on the retrieved documents, and the resulting answers are evaluated by the LLM-as-a-Judge. This evaluation assesses how effectively each RAG pipeline retrieves and synthesizes information from the source corpus, as reflected in the accuracy scores provided by the LLM evaluator.

Table~\ref{tab:llm_eval_clean} reports results for a set of models differing in architecture. \texttt{gpt-oss-20b} attains the highest accuracy on both single-hop (58.85\%) and multi-hop (18.86\%) questions. \texttt{Llama-3.3-70B-Instruct} (53.53\% single-hop, 17.92\% multi-hop) and \texttt{gpt-4o-mini} (52.72\% single-hop, 13.77\% multi-hop) follow in performance. \texttt{Llama-3.3-3b-Instruct} shows the lowest performance on both tasks, with 24.10\% on single-hop and 3.2\% on multi-hop questions.

The gap between single- and multi-hop performance underscores the persistent challenge of compositional reasoning in retrieval-augmented generation. While single-hop accuracy varies significantly (ranging from 24.10\% to 58.85\%), scores drop substantially on multi-hop questions for all models. This pattern suggests that retrieval quality alone is insufficient: the answering step requires analyzing multiple and potentially long chunks. Effective integration and reasoning across multiple contexts remain critical limiting factors.

\section{Conclusion and Future Works}

We introduced a French-language dataset of \nSHqueries ~\ single-hop and \nMHqueries ~\ multi-hop questions from French Third Republic parliamentary debates and newspapers, designed to advance RAG system evaluation for historical research. Our historian-guided methodology ensures questions reflect genuine historical inquiry.

While single-hop retrieval performs well, multi-hop reasoning and cross-source synthesis remain challenging. Our experiments used Cohere API models for their benchmark performance. Future work will compare alternative models across question generation, retrieval, reranking, and LLM evaluation; expand the dataset; and refine RAG architectures to improve cross-source reasoning and assess model robustness.

\begin{comment}
In this paper, we introduced a novel French-language dataset comprising 897 single-hop and 885 multi-hop questions derived from parliamentary debates and newspapers of the French Third Republic. The dataset is designed to advance the development and evaluation of Retrieval-Augmented Generation (RAG) systems in the context of historical research. Our historian-guided methodology ensures that the generated questions are both accurate and meaningful, reflecting the depth and complexity of genuine historical inquiry.

While single-hop retrieval achieves strong performance, multi-hop reasoning and cross-source synthesis continue to pose significant challenges, highlighting key areas for future investigation. 

 Our initial experiments relied on models from the Cohere API, chosen for their strong performance on various benchmarks. 
 As future work We intend to extend this evaluation by comparing question generation, retrieval, reranking, and LLM-as-a-judge setups across alternative models. We also plan to expand the dataset, diversify question types, and refine RAG architectures to improve cross-source integration and reasoning capabilities to further assess model robustness and adaptability.
\end{comment}


\section{Limitations}

We relied on Cohere API models for question generation, retrieval, and alignment evaluation, which means results may differ with alternative models. In addition, the question generation and manual validation were performed by a single historian; future work would benefit from multiple annotators to assess inter-annotator agreement. Furthermore, the corpus is limited to a single year (1887) and two newspapers, which may not capture the full diversity of the period. Finally, our LLM-as-a-judge alignment was validated on a small subset (50 questions).

\section{Ethical Considerations}

The historical documents used in our corpus are in the public domain, sourced from the Bibliothèque nationale de France. These texts contain viewpoints and biases from their period (1887), which the dataset is designed to capture for research purposes, not to endorse. To mitigate the risks associated with historically biased or misleading interpretations, we adopted a historian-in-the-loop methodology for question validation.

\section{Bibliographical References}\label{sec:reference}
\bibliographystyle{lrec2026-natbib}
\bibliography{lrec2026-example}


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
